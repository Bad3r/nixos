# Logseq Git-Only Flake Plan

_Last updated: 2025-10-05_

## Mandated Outcomes

- Pure Nix, self-contained flake with no external scripts.
- GitHub Actions performs nightly Logseq builds at 00:00 UTC, publishes fully built Electron bundles (the `Logseq-linux-x64` output compressed as an artifact) as GitHub Releases, and records metadata (release tag, upstream rev, SHA256).
- The same workflow updates a manifest in the repo with the new hash/rev and then runs validation jobs.
- Flake provides tooling to download/sync the latest release artifact, verify integrity, and expose Logseq via an FHS wrapper plus `.desktop` entry and binary on `$PATH`—mirroring a nixpkgs-style package.
- A systemd service and timer generated by the flake check daily for new releases and download/install them when available, defaulting to 02:00 UTC (two hours after the nightly action).
- Logging uses structured JSON with `info`, `warn`, and `debug` levels for all sync/download activities.
- Linux-only target; offline operation is out of scope.
- The flake remains generic: no references to this repository’s productivity role—host wiring happens externally.

## Phase 0 — Strategy & Risk Validation

- [ ] Finalise the GitHub repository `nix-logseq-git-flake` layout, including permissions for publishing GitHub Releases via Actions (use `gh` tooling as needed).
- [ ] Specify release naming/versioning scheme (e.g., `nightly-YYYYMMDD`) and retention policy; ensure automation can prune old releases if necessary.
- [ ] Design the release workflow so that it builds the Logseq Electron bundle, archives only the runtime output (`Logseq-linux-x64` directory), uploads it as a GitHub Release asset, computes SHA256/upstream rev, writes the manifest JSON, commits it, and triggers validation jobs in the same workflow (all timestamps recorded in UTC). Schedule the workflow to run daily at 00:00 UTC.
- [ ] Document host prerequisites: ability to run `nix run`/`nix build` from systemd services, network access to fetch GitHub Releases, and permission to manage systemd user timers.

## Phase 1 — Current-State Discovery

- [ ] Catalogue existing Logseq automation (`modules/apps/logseq.nix`, timers) to preserve launch behaviour and user experience.
- [ ] Analyse `modules/devshell/logseq.nix` to extract runtime dependencies for the eventual FHS wrapper (build deps no longer required locally).
- [ ] Examine git-based sync logic in `modules/hm-apps/ghq-mirror.nix` for divergence/backups should metadata tracking need safe resets.
- [ ] Review multiple Electron packages in `/home/vx/git/nixpkgs` for runtime packaging patterns (wrapper, desktop entries, icons).
- [ ] Study other download-based packages in nixpkgs for best practices on verifying external artifacts.
- [ ] Inspect `sss-update-logseq` outputs to align release installation paths with user expectations.

## Phase 2 — Interface & Option Schema Design

- [ ] Define flake outputs (`packages`, `apps`, `nixosModules`, `checks`) and document interfaces.
- [ ] Design module options for manifest path (default under `data/logseq-nightly.json`), download/cache directories, systemd unit names, logging level defaults, and user/group ownership.
- [ ] Specify manifest JSON structure (release tag, asset URL, SHA256, upstream rev, UTC timestamp) updated by the release workflow.
- [ ] Outline integration points for consuming NixOS configs (how hosts map options without referencing productivity role specifics).

## Phase 3 — Release Manifest & Metadata Pipeline

- [ ] Implement the manifest and helper functions to load/validate it inside the flake (schema checks, structured error reporting including UTC timestamps).
- [ ] Prototype hash verification utilities consuming manifest data and emitting JSON logs at info/warn/debug levels.
- [ ] Define how manifest metadata is exposed to downstream packages (library helpers or module arguments).

## Phase 4 — Logseq Wrapper Packaging

- [ ] Identify required runtime libraries by studying relevant Electron packages in nixpkgs; document the final list.
- [ ] Implement a derivation that downloads the release artifact (`fetchzip`) using manifest metadata and installs the runtime bundle into the Nix store.
- [ ] Wrap the binary in an FHS environment with the runtime libraries; ensure the `logseq` command is exported on `$PATH` and a `.desktop` entry/icon are generated.
- [ ] Add smoke tests (e.g., `nix run`) suitable for CI.

## Phase 5 — Sync Helper Implementation

- [ ] Design configurable state/cache directory layout (default under `/var/lib/logseq` or XDG-compliant user path) and locking strategy for downloads.
- [ ] Implement a `writeShellApplication` that fetches the release asset, verifies SHA256, extracts to the state directory, emits JSON logs, and skips when version unchanged.
- [ ] Ensure error handling covers partial downloads, hash mismatch, network failures, and produces meaningful warnings/errors (with UTC timestamps in logs).
- [ ] Expose the helper as both a package (`packages.logseq-sync`) and an app (`apps.logseq-sync`).
- [ ] Create integration tests simulating manifest changes and running the helper to validate behaviour.

## Phase 6 — Systemd Integration Module

- [ ] Implement `nixosModules.logseq` that installs the wrapper package, configures a systemd service running the sync helper via `nix run`, and defines a timer defaulting to 02:00 UTC (two hours after the GitHub Action) with configurable schedule.
- [ ] Configure logging (journal identifier, JSON output) and state directory permissions through module options.
- [ ] Provide module option documentation, ensuring no references to repo-local concepts.
- [ ] Add NixOS tests or scripted checks (if feasible) verifying service/timer definitions and successful sync.

## Phase 7 — GitHub Actions Workflows

- [ ] Implement the build-and-release job that packages the Electron bundle, uploads it to GitHub Releases, computes SHA256/upstream rev, and attaches metadata (recorded with UTC timestamps).
- [ ] Implement the manifest-update-and-validate job that consumes the metadata, updates `data/logseq-nightly.json`, runs `nix fmt`/`nix flake check`, commits/pushes changes, and executes integration tests (sync helper, timer simulation as appropriate).
- [ ] Configure a push-triggered workflow reusing the validation job to guard contributions.
- [ ] Set secrets/permissions required for release publishing and repository pushes.

## Phase 8 — Integration Back Into This Repository

- [ ] Add the external flake input to this repo’s `flake.nix` and lock it.
- [ ] Replace local Logseq module/package references with imports from the external flake, mapping host options (manifest path, cache directory, logging level) without mentioning productivity role specifics.
- [ ] Remove or disable legacy ghq/build scripts once new workflow validated.
- [ ] Ensure host profiles use the new `logseq` wrapper command and desktop entry.

## Phase 9 — Validation & Documentation

- [ ] Run `nix flake check --accept-flake-config` on the external flake and this repo after integration.
- [ ] Manually trigger the sync service to confirm release download, hash verification, and JSON logging behaviour.
- [ ] Validate timer scheduling by starting the timer and observing service execution; ensure logs reflect info/warn/debug levels and record UTC timestamps.
- [ ] Build a representative host closure verifying end-to-end functionality.
- [ ] Update `docs/logseq-local-workflow.md` with the new release-based architecture, manifest location, JSON logging, and CI responsibilities (no release notes/migration guide required).

## Resolved Questions

- Build hooks: fully encapsulated in CI; local flake only downloads release artifacts.
- Remote support: GitHub-only (official Logseq repo).
- Multi-user deployments: out of scope.
- Logging: JSON-formatted info/warn/debug levels in sync helper.

## Open Follow-ups

- [ ] Decide retention/cleanup policy for GitHub Releases (e.g., keep last N or prune automatically).
